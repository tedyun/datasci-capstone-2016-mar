line <- lineNumbers[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
}
loveLines <- getLinesContainingString(dataset_twitter, 'love')
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- dataset[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
}
loveLines <- getLinesContainingString(dataset_twitter, 'love')
loveLines
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- dataset[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
return(lineNumbers)
}
loveLines
loveLines <- getLinesContainingString(dataset_twitter, 'love')
loveLines
length(loveLines)
hateLines <- getLinesContainingString(dataset_twitter, 'hate')
length(loveLines) / length(hateLines)
biostatLines <- getLinesContainingString(dataset_twitter, 'biostats')
biostatLines
dataset_twitter[556872]
kickboxing <- getLinesContainingString(dataset_twitter, "A computer once beat me at chess, but it was no match for me at kickboxing")
kickboxing
readFileLines <- function (sFilename) {
conn = file(sFilename, open = "r")
result <- readLines(conn)
close(conn)
return(result)
}
blogs <- readFileLines("final/en_US/en_US.blogs.txt")
news <- readFileLines("final/en_US/en_US.news.txt")
twitter <- readFileLines("final/en_US/en_US.twitter.txt")
blogLines <- nchar(blogs)
newsLines <- nchar(news)
twitterLines <- nchar(twitter)
tokenizeLine("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?")
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
tokenizeLine <- function (sLine) {
unlist(strsplit(sLine, splitRegex, fixed = FALSE))
}
tokenizeLine("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?")
strsplit("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?", subLineRegex)
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
c(1,2)
len(c(1,2))
length(c(1,2))
length(c(1,2,3,4))
1:%
1:5
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[i] <- tokenizeSubline(subLines)
}
tokList
}
twitter[[1]]
tokenizeLine(twitter[[1]])
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[i] <- tokenizeSubline(subLines[i])
}
tokList
}
tokenizeLine(twitter[[1]])
c(1,2)[1]
c(1,2)[2]
a <- list()
a[1] <- 1
a[2] <- 2
a
a
a[[1]] <- 1
a[[2]] <- 2
a
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[[i]] <- tokenizeSubline(subLines[i])
}
tokList
}
tokenizeLine(twitter[[1]])
twitter[[1]]
twitter[1]
twitter[[1]]
twitter[1]
tokenizeLine(twitter[1])
tokenized <- tokenizeLine(twitter[1])
tokenized
length(tokenized)
tokenized[[1]]
getNumberOfWordsInLine <- function (sLine) {
tokenized <- tokenizeLine(sLine)
len <- 0
for (i in 1: tokenized){
subLine <- tokenized[[i]]
len <- len + length(subLine)
}
len
}
getNumberOfWordsInLine(twitter[1])
getNumberOfWordsInLine <- function (sLine) {
tokenized <- tokenizeLine(sLine)
len <- 0
for (i in 1:length(tokenized)){
subLine <- tokenized[[i]]
len <- len + length(subLine)
}
len
}
getNumberOfWordsInLine(twitter[1])
twitter[1]
newsWords <- sapply(news, getNumberOfWordsInLine)
head(newsWords)
newsWords[[1]]
newsWords[[2]]
newsWords[[3]]
newsWords[3]
newsWords <- vapply(news, getNumberOfWordsInLine)
newsWords <- vapply(news, getNumberOfWordsInLine, 1)
head(newsWords)
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE)
head(newsWords)
newsWords[1]
newsWords[[1]]
news[[1]]
news[[2]]
news[1]
head(news)
getNumberOfWordsInLine(news[1])
summary(newsWords)
str(newsWords)
newsWords[1]
newsWords[[1]
]
?apply
?sapply
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE>NAMES = FALSE)
ords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
head(newsWords)
newsWords[2]
news[2]
blogWords <- sapply(blogs, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
twitterWords <- sapply(twitter, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
?saveRDS
saveRDS(blogWords, "blogWords.rds")
saveRDS(twitterWords, "twitterWords.rds")
saveRDS(newsWords, "newsWords.rds")
saveRDS(blogs, "blogs.rds")
saveRDS(twitter, "twitter.rds")
saveRDS(news, "news.rds")
summary(blogWords)
summary(twitterWords)
summary(newsWords)
?sapply
newsTokenized <- lapply(news, tokenizeLine)
newsTokenized[1]
newsTokenized[[1]
]
newsTokenized[2]
news[1]
news[2]
summary(news)
str(news)
newsTokenized[[2]
]
newsTokenized[[3]]
str(newsTokenized[[3]])
blogTokenized <- lapply(blogs, tokenizeLine)
?is.na
dict <- list()
dict
dict[["abc"]] <- 0
dict
k <- "abc"
dict[[k]]
dict[["def"]]
is.na(dict[["def"]])
dict[["def"]] == NULL
is.null(dict[["def"]])
createDictionary <- function (listTokenized) {
dict <- list()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
dict
}
createDictionary(newsTokenized)
newsTokenized[[1]][[1]][1]
length(newsTokenized)
length(news)
length(newsTokenized[[1]])
length(newsTokenized[[1]][[1]])
tolower(newsTokenized[[1]][[1]][1])
dict <- alist()
dict <- list()
sWords <- "he"
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
sWord <- "he"
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
dict
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
dict
sWord <- "she"
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
dict
createDictionary <- function (listTokenized) {
dict <- list()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
dict
}
createDictionary(newsTokenized)
tolower("He")
createDictionary <- function (listTokenized) {
dict <- list()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
print(paste(i, j, k))
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
dict
}
createDictionary(newsTokenized)
sWord <- "He"
sWord
length(sWord)
nchar(sWord)
tolower("")
countWordFrequency <- function (listTokenized) {
dict <- list()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
if (nchar(sWord) > 0) {
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
}
dict
}
newsWordFreq <- countWordFrequency(newsTokenized)
newsWordFreq <- createDictionary(newsTokenized)
countWordFrequency <- function (listTokenized) {
dict <- list()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
if (length(sWord) > 0 && nchar(sWord) > 0) {
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
}
dict
}
newsWordFreq <- countWordFrequency(newsTokenized)
install.packages("hash")
library(hash)
countWordFrequency <- function (listTokenized) {
dict <- hash()
for (i in 1:length(listTokenized)) {
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
if (length(sWord) > 0 && nchar(sWord) > 0) {
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
}
dict
}
5%4
5 %% 4
countWordFrequency <- function (listTokenized) {
dict <- hash()
for (i in 1:length(listTokenized)) {
if (i %% 10000 == 0) {
print (paste(i, "-th line processed."))
}
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- tolower(sVec[k])
if (length(sWord) > 0 && nchar(sWord) > 0) {
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
}
dict
}
newsWordFreqHash <- countWordFrequency(newsTokenized)
names(newsWordFreqHash)
head(names(newsWordFreqHash))
newsWordFreqHash[["he"]]
newsWordFreqHash[["she"]]
newsWordFreqHash[["like"]]
length(names(newsWordFreqHash))
twitterTokenized <- lapply(twitter, tokenizeLine)
saveRDS(blogTokenized, "blogTokenized.rds")
saveRDS(newsTokenized, "newsTokenized.rds")
saveRDS(twitterTokenized, "twitterTokenized.rds")
head(value(newsWordFreqHash))
head(values(newsWordFreqHash))
val <- values(newsWordFreqHash)
str(val)
val[1]
val["he"]
val["she"]
val["he's"]
val <- sort(val, decreasing=TRUE)
head(val)
blogWordFreqHash <- countWordFrequency(blogTokenized)
saveRDS(blogLines, "blogLines.rds")
saveRDS(newsLines, "newsLines.rds")
saveRDS(twitterLines, "twitterLines.rds")
newsLines <- readRDS("newsLines.rds")
summary(newsLines)
newsLines <- nchar(news)
summary(newsLines)
summary(blogLines)
summary(twitterLines)
231.7/41.97
203/34.98
68.8/12.96
?st.dev
??st.dev
?stdev
st_dev?
fd
f
f
?st_dev
?var
sd(blogWords)
sd(newsWords)
sd(twitterWords)
hist(newsWords)
hist(twitterWords)
hist(blogWords)
?hist
hist(blogWords, freq=FALSE, xlab="", ylab="", main="")
hist(blogWords, freq=TRUE, xlab="", ylab="", main="")
hist(blogWords, freq=FALSE, xlab="", ylab="", main="")
hist(blogWords, breaks=100, xlab="", ylab="", main="")
max(blogWords)
head(blogWords)
hist(blogWords, breaks=100, xlab="", ylab="", main="", ylim=1000)
?ylim
hist(blogWords, breaks=100, xlab="", ylab="", main="", xlim=c(0,1000)
)
hist(blogWords, breaks=100, xlab="", ylab="", main="", xlim=c(0, 300))
?hist
hist(blogWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 300))
hist(blogWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 250))
hist(newsWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 250))
hist(blogWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 250))
hist(newsWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 150))
hist(twitterWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 150))
hist(twitterWords, breaks=1000, xlab="", ylab="", main="", xlim=c(0, 50))
hist(twitterWords, breaks=100, xlab="", ylab="", main="", xlim=c(0, 50))
hist(twitterWords, breaks=100, xlab="", ylab="", main="", xlim=c(0, 35))
hist(twitterWords, breaks=50, xlab="", ylab="", main="", xlim=c(0, 35))
saveRDS(newsWordFreqHash, "newsWordFreqHash.rds")
getTopNWords <- function (wordFreqHash, N) {
val <- values(wordFreqHash)
sort(val, decreasing=TRUE)
val[1:N]
}
newsTop20 <- getTopNWords(newsWordFreqHash, 20)
newsTop20
getTopNWords <- function (wordFreqHash, N) {
val <- values(wordFreqHash)
val1 <- sort(val, decreasing=TRUE)
val1[1:N]
}
newsTop20 <- getTopNWords(newsWordFreqHash, 20)
newsTop20
newsTop50 <- getTopNWords(newsWordFreqHash, 50)
newsTop50
newsTop50("NA")
newsTop50[["NA"]]
tolower("123")
is.na("abc")
a <- NA
a
is.na(a)
is.na(a) && TRUE
is.na(a) && FALSE
length(a)
nchar(a)
length(a) > 0
nchar(a) > 0
countWordFrequency <- function (listTokenized) {
dict <- hash()
for (i in 1:length(listTokenized)) {
if (i %% 10000 == 0) {
print (paste(i, "-th line processed."))
}
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
for (k in 1:length(sVec)){
sWord <- sVec[k]
if (!is.na(sWord) && length(sWord) > 0 && nchar(sWord) > 0) {
sWord <- tolower(sWord)
if (length(sWord) > 0 && nchar(sWord) > 0) {
dict[[sWord]] <- if (is.null(dict[[sWord]])) 1 else dict[[sWord]] + 1
}
}
}
}
}
dict
}
newsWordFreqHash <- countWordFrequency(newsTokenized)
newsTop100 <- getTopNWords(newsWordFreqHash, 10
ljaklsdfasdf)
newsTop100 <- getTopNWords(newsWordFreqHash, 100)
newsTop100
newsTop50 <- getTopNWords(newsWordFreqHash, 50)
newsTop50
saveRDS(newsTop50, "newsTop50.rds")
length(keys(newsWordFreqHash))
