?readLines
con <- file("final/en_US.twitter.txt", "r")
readLines(con)
close(con)
con <- file("final/en_US/en_US.twitter.txt", "r")
readLines(con)
close(con)
con <- file("final/en_US/en_US.twitter.txt", "r")
dataset_twitter <- readLines(con)
close(con)
dim(dataset_twitter)
nrow(dataset_twitter)
summary(dataset_twitter)
con <- file("final/en_US/en_US.news.txt", "r")
dataset_news <- readLines(con)
close(con)
con <- file("final/en_US/en_US.blogs.txt", "r")
dataset_blogs <- readLines(con)
close(con)
dataset_twitter[0]
dataset_twitter[[0][]
dataset_twitter[[0]
]
dataset_twitter$1
dataset_twitter[[1]]
dataset_twitter[1]
str(dataset_twitter)
length(dataset_twitter)
length(dataset_twitter[1])
dataset_twitter[1]
nchar(dataset_twitter[1])
getMaximumLineLenght <- function (dataset) {
maxLength <- 0
for (line in dataset) {
curLength <- nchar(line)
if (curLength > maxLength) {
maxLength <- curLength
}
}
}
getMaximumLineLenght(dataset_twitter)
getMaximumLineLength <- function (dataset) {
maxLength <- 0
for (line in dataset) {
curLength <- nchar(line)
if (curLength > maxLength) {
maxLength <- curLength
}
}
return(maxLength)
}
getMaximumLineLenght(dataset_twitter)
getMaximumLineLength(dataset_twitter)
getMaximumLineLength(dataset_news)
getMaximumLineLength(dataset_blogs)
head(dataset_twitter)
line <- "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason."
strsplit(line, " ")
strsplit(line, "[ \.]")
strsplit(line, "[ .]")
strsplit(line, "[^a-zA-Z0-9']")
?Filter
TRUE || FALSE
TRUE | FALSE
TRUE && FALSE
TRUE & FALSE
"a" == "a"
"a" == "b"
tokenizeLine <- function (line) {
tokens <- strsplit(line, "[^a-zA-Z0-9']")
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 0) && (tolower(word) != "a"))
) {
return(TRUE)
}
return(FALSE)
}
return(Filter(tokenFilter, tokens))
}
tokenizeLine(line)
tokenizeLine <- function (line) {
tokens <- strsplit(line, "[^a-zA-Z0-9']")
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 0) && (tolower(word) != "a"))
) {
return(FALSE)
}
return(TRUE)
}
return(Filter(tokenFilter, tokens))
}
tokenizeLine(line)
nchar("")
line
tokenizeLine <- function (line) {
tokens <- strsplit(line, "[^a-zA-Z0-9']")
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 1) && (tolower(word) != "a"))
) {
return(FALSE)
}
return(TRUE)
}
return(Filter(tokenFilter, tokens))
}
tokenizeLine(line)
tokenizeLine <- function (line) {
tokens <- strsplit(line, "[^a-zA-Z0-9'\-]")
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 1) && (tolower(word) != "a"))
) {
return(FALSE)
}
return(TRUE)
}
return(Filter(tokenFilter, tokens))
}
tokenizeLine <- function (line) {
tokens <- strsplit(line, "[^a-zA-Z0-9'\-]")
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 1) && (tolower(word) != "a"))
) {
return(FALSE)
}
return(TRUE)
}
return(Filter(tokenFilter, tokens))
}
strsplit(line, "[^a-zA-Z0-9'\-]")
strsplit(line, "[^a-zA-Z0-9'-]")
strsplit("abc def-ghi - gji asdfll asdfjklads fdasklaf", "[^a-zA-Z0-9'-]")
strsplit(line, "[^a-zA-Z0-9'-]")
unlist(strsplit(line, "[^a-zA-Z0-9'-]"))
tokenizeLine <- function (line) {
tokens <- unlist(strsplit(line, "[^a-zA-Z0-9'-]"))
tokenFilter <- function (word) {
if (
(nchar(word) == 0) ||
((nchar(word) == 1) && (tolower(word) != "a"))
) {
return(FALSE)
}
return(TRUE)
}
return(Filter(tokenFilter, tokens))
}
tokenizeLine(line)
"When" %in% tokenizeLine(line)
getNumberOfLinesWithWord <- function (dataset, word) {
count <- 0
for (line in dataset) {
tokens <- tokenizeLine(line)
if (word %in% tokens) {
count <- count + 1
}
}
return(maxLength)
}
getNumberOfLinesWithWord(dataset_twitter, "love")
getNumberOfLinesWithWord <- function (dataset, word) {
count <- 0
for (line in dataset) {
tokens <- tokenizeLine(line)
if (word %in% tokens) {
count <- count + 1
}
}
return(count)
}
getNumberOfLinesWithWord(dataset_twitter, "love")
c()
c(NULL, 'a')
c(c(NULL, 'a'), 'b')
1:5
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- lineNumbers[i]
if (grep(keyword, line, fixed=TRUE)) {
lineNumbers <- c(lineNumbers, i)
}
}
}
loveLines <- getLinesContainingString(dataset_twitter, 'love')
grep('a', 'abc')
grep('d', 'abc')
length(grep('d', 'abc'))
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- lineNumbers[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
}
loveLines <- getLinesContainingString(dataset_twitter, 'love')
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- dataset[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
}
loveLines <- getLinesContainingString(dataset_twitter, 'love')
loveLines
getLinesContainingString <- function (dataset, keyword) {
lineNumbers <- NULL
for (i in 1:length(dataset)) {
line <- dataset[i]
if (length(grep(keyword, line, fixed=TRUE)) > 0) {
lineNumbers <- c(lineNumbers, i)
}
}
return(lineNumbers)
}
loveLines
loveLines <- getLinesContainingString(dataset_twitter, 'love')
loveLines
length(loveLines)
hateLines <- getLinesContainingString(dataset_twitter, 'hate')
length(loveLines) / length(hateLines)
biostatLines <- getLinesContainingString(dataset_twitter, 'biostats')
biostatLines
dataset_twitter[556872]
kickboxing <- getLinesContainingString(dataset_twitter, "A computer once beat me at chess, but it was no match for me at kickboxing")
kickboxing
readFileLines <- function (sFilename) {
conn = file(sFilename, open = "r")
result <- readLines(conn)
close(conn)
return(result)
}
blogs <- readFileLines("final/en_US/en_US.blogs.txt")
news <- readFileLines("final/en_US/en_US.news.txt")
twitter <- readFileLines("final/en_US/en_US.twitter.txt")
blogLines <- nchar(blogs)
newsLines <- nchar(news)
twitterLines <- nchar(twitter)
tokenizeLine("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?")
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
tokenizeLine <- function (sLine) {
unlist(strsplit(sLine, splitRegex, fixed = FALSE))
}
tokenizeLine("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?")
strsplit("Hi how are you - I'm good -- I'm very good.Let's talk about issues!!?! \"ok let's do that...\" What do-you want to talk about?", subLineRegex)
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
c(1,2)
len(c(1,2))
length(c(1,2))
length(c(1,2,3,4))
1:%
1:5
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[i] <- tokenizeSubline(subLines)
}
tokList
}
twitter[[1]]
tokenizeLine(twitter[[1]])
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[i] <- tokenizeSubline(subLines[i])
}
tokList
}
tokenizeLine(twitter[[1]])
c(1,2)[1]
c(1,2)[2]
a <- list()
a[1] <- 1
a[2] <- 2
a
a
a[[1]] <- 1
a[[2]] <- 2
a
subLineRegex = "( - )|( -- )|([.!?\":;]+[ ]*[.!?\":;]*)"
splitRegex = "[-,\t\n\r ]+"
splitIntoSubLines <- function (sLine) {
unlist(strsplit(sLine, subLineRegex, fixed = FALSE))
}
tokenizeSubline <- function (sSubLine) {
unlist(strsplit(sSubLine, splitRegex, fixed = FALSE))
}
tokenizeLine <- function (sLine) {
tokList <- list()
subLines <- splitIntoSubLines(sLine)
for (i in 1:length(subLines)) {
tokList[[i]] <- tokenizeSubline(subLines[i])
}
tokList
}
tokenizeLine(twitter[[1]])
twitter[[1]]
twitter[1]
twitter[[1]]
twitter[1]
tokenizeLine(twitter[1])
tokenized <- tokenizeLine(twitter[1])
tokenized
length(tokenized)
tokenized[[1]]
getNumberOfWordsInLine <- function (sLine) {
tokenized <- tokenizeLine(sLine)
len <- 0
for (i in 1: tokenized){
subLine <- tokenized[[i]]
len <- len + length(subLine)
}
len
}
getNumberOfWordsInLine(twitter[1])
getNumberOfWordsInLine <- function (sLine) {
tokenized <- tokenizeLine(sLine)
len <- 0
for (i in 1:length(tokenized)){
subLine <- tokenized[[i]]
len <- len + length(subLine)
}
len
}
getNumberOfWordsInLine(twitter[1])
twitter[1]
newsWords <- sapply(news, getNumberOfWordsInLine)
head(newsWords)
newsWords[[1]]
newsWords[[2]]
newsWords[[3]]
newsWords[3]
newsWords <- vapply(news, getNumberOfWordsInLine)
newsWords <- vapply(news, getNumberOfWordsInLine, 1)
head(newsWords)
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE)
head(newsWords)
newsWords[1]
newsWords[[1]]
news[[1]]
news[[2]]
news[1]
head(news)
getNumberOfWordsInLine(news[1])
summary(newsWords)
str(newsWords)
newsWords[1]
newsWords[[1]
]
?apply
?sapply
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE>NAMES = FALSE)
ords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
newsWords <- sapply(news, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
head(newsWords)
newsWords[2]
news[2]
blogWords <- sapply(blogs, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
twitterWords <- sapply(twitter, getNumberOfWordsInLine, simplify=TRUE, USE.NAMES = FALSE)
?saveRDS
saveRDS(blogWords, "blogWords.rds")
saveRDS(twitterWords, "twitterWords.rds")
saveRDS(newsWords, "newsWords.rds")
saveRDS(blogs, "blogs.rds")
saveRDS(twitter, "twitter.rds")
saveRDS(news, "news.rds")
summary(blogWords)
summary(twitterWords)
summary(newsWords)
