sWord2 <- sVec[k + 1]
sWord3 <- sVec[k + 2]
if (isString(sWord1) && isString(sWord2) && isString(sWord3)) {
n1 <- wordToIndexDict[[tolower(sWord1)]]
n2 <- wordToIndexDict[[tolower(sWord2)]]
n3 <- wordToIndexDict[[tolower(sWord3)]]
## print(paste(sWord1, sWord2, sWord3))
if (!is.null(n1) && !is.null(n2) && !is.null(n3)) {
## find the row
#             found <- FALSE
#             for (idx in 1:nrow(mat)) {
#               if (identical(mat[idx,][1:3], row)) {
#                 found <- TRUE
#                 break
#               }
#             }
#             if (!found) {
#               idx <- -1
#             }
## idx <- findRow3(mat, c(n1, n2, n3), curRow)
## idx <- which(mat[,1]==n1 & mat[,2] == n2 & mat[,3] == n3, arr.ind = TRUE)
hashkey <- paste(n1, n2, n3)
idx <- hashIndex[[hashkey]]
if (!is.null(idx)) {
mat[idx, 4] <- mat[idx, 4] + 1
} else {
if (curRow > nrow(mat)){
print("Growing matrix..")
rowN <- nrow(mat)
colN <- ncol(mat)
mat <- rbind(mat, matrix(NA, nrow=rowN, ncol=colN))
print(paste("Matrix grown.", nrow(mat)))
}
# Growing 1 row
mat[curRow,] <- c(n1, n2, n3, 1)
hashIndex[[hashkey]] <- curRow
curRow <- curRow + 1
}
}
}
}
}
}
mat
}
matTrained <- readRDS("matTrained_news2000.rds")
matTrained <- trainMatrix(matTrained, newsTokenized[2001:3000], wordToIndexDict)
saveRDS(matTrained, "matTrained_news3000.rds")
predict <- function (mat, sWord1, sWord2, wordToIndexDict, indexToWordVect) {
sub <- mat[which(mat[,1] == wordToIndexDict[[sWord1]] & mat[,2] == wordToIndexDict[[sWord2]], arr.ind=TRUE),]
sub <- data.frame(sub[,3:4])
sub <- sub[order(-sub[2]),]
sub[1] <- sapply(sub[1], function (idx) { indexToWordVect[idx] }, USE.NAMES = FALSE)
return(sub)
}
predict(matTrained, "couple", "of", wordToIndexDict, indexToWordVect)
trainMatrix <- function (mat, listTokenized, wordToIndexDict) {
# find the current row
for (curRow in 1:nrow(mat)) {
if (is.na(mat[curRow, 1])) {
break
}
}
print(paste("Starting at", curRow, "th row"))
print("Building index..")
hashIndex <- hash()
for (i in 1:(curRow - 1)) {
key <- paste(mat[i, 1], mat[i, 2], mat[i, 3])
hashIndex[[key]] <- i
}
print("Index built.")
for (i in 1:length(listTokenized)) {
if (i %% 1000 == 0) {
print (paste(i, "-th line trained."))
}
sublist <- listTokenized[[i]]
for (j in 1:length(sublist)) {
sVec <- sublist[[j]]
if (length(sVec) < 3) {
next
}
for (k in 1:(length(sVec) - 2)){
sWord1 <- sVec[k]
sWord2 <- sVec[k + 1]
sWord3 <- sVec[k + 2]
if (isString(sWord1) && isString(sWord2) && isString(sWord3)) {
n1 <- wordToIndexDict[[tolower(sWord1)]]
n2 <- wordToIndexDict[[tolower(sWord2)]]
n3 <- wordToIndexDict[[tolower(sWord3)]]
## print(paste(sWord1, sWord2, sWord3))
if (!is.null(n1) && !is.null(n2) && !is.null(n3)) {
## find the row
#             found <- FALSE
#             for (idx in 1:nrow(mat)) {
#               if (identical(mat[idx,][1:3], row)) {
#                 found <- TRUE
#                 break
#               }
#             }
#             if (!found) {
#               idx <- -1
#             }
## idx <- findRow3(mat, c(n1, n2, n3), curRow)
## idx <- which(mat[,1]==n1 & mat[,2] == n2 & mat[,3] == n3, arr.ind = TRUE)
hashkey <- paste(n1, n2, n3)
idx <- hashIndex[[hashkey]]
if (!is.null(idx)) {
mat[idx, 4] <- mat[idx, 4] + 1
} else {
if (curRow > nrow(mat)){
print("Growing matrix..")
rowN <- nrow(mat)
colN <- ncol(mat)
mat <- rbind(mat, matrix(NA, nrow=rowN, ncol=colN))
print(paste("Matrix grown.", nrow(mat)))
}
# Growing 1 row
mat[curRow,] <- c(n1, n2, n3, 1)
hashIndex[[hashkey]] <- curRow
curRow <- curRow + 1
}
}
}
}
}
}
mat
}
matTrained <- trainMatrix(matTrained, newsTokenized[3001:10000], wordToIndexDict)
saveRDS(matTrained, "matTrained_news10000.rds")
matTrained <- trainMatrix(matTrained, newsTokenized[10001:20000], wordToIndexDict)
saveRDS(matTrained, "matTrained_news20000.rds")
predict(matTrained, "case", "of", wordToIndexDict, indexToWordVect)
predict(matTrained, "mean", "the", wordToIndexDict, indexToWordVect)
predict(matTrained, "me", "the", wordToIndexDict, indexToWordVect)
matTrained <- trainMatrix(matTrained, newsTokenized[20001:30000], wordToIndexDict)
saveRDS(matTrained, "matTrained_news30000.rds")
source("newAnalysis.R")
rm(ls())
rm(list=ls())
source("newAnalysis.R")
source("newAnalysis.R")
source("newAnalysis.R")
source("newAnalysis.R")
head(blogs)
blogs[1]
rm(list=ls())
?sample
?sample
?sample
sample(1:10, size=3)
v <- sample(1:10, size=3)
1:10[-v]
v
1:10[v]
w <- 1:10
w[v]
w[-v]
?createDataPartition
??createDataPartition
install.packages("caret")
??createDataPartition
v <- createDataPartition(1:10, p=0.8, list=FALSE)
v
library(caret)
v <- createDataPartition(1:10, p=0.8, list=FALSE)
v
library(quanteda)
sample(1:10, size = 0.8*10)
sample(1:10, size = 5.4)
sample(10, size = 5.4)
sample(10, size = 3)
sample(10, size = 3)
sample(10, size = 3)
sample(10, size = 3)
source("newAnalysis.R")
length(blog)
length(blogTrainIndex)
head(blogTrainIndex)
head(sort(blogTrainIndex))
tail(sort(blogTrainIndex))
createSamples <- function (totalNum, ratio){
sort(sample(totalNum, size = totalNum * ratio))
}
blogTrainIndex <- createSamples(length(blog), 0.8)
newsTrainIndex <- createSamples(length(blog), 0.8)
twitterTrainIndex <- createSamples(length(blog), 0.8)
head(blogTrainIndex)
tail(blogTrainIndex)
source("newAnalysis.R")
blog[1]
trainBlog[1]
testBlog[1]
blog[2]
blog[3]
trainBlog[3]
trainBlog[2]
shuffle(3)
??shuffle
install.packages("permute")
library("permute")
shuffle(3)
shuffle(4)
shuffle(4)
rm(list=ls())
source("newAnalysis.R")
(length(news) + length(blog) + length(twitter))/8
(length(news) + length(blog) + length(twitter))*0.8
trainingData[1]
saveRDS(trainingData, "trainingData.rds")
source("newAnalysis.R")
?dfm
trainingDFM <- dfm(trainingData[1:1000], toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
trainingDFM
trainingDFM[1]
print(trainingDFM[1])
print(trainingDFM[1], show.values=TRUE)
trainintData[1]
trainingData[1]
trainingDF[1][2]
trainingDFM[1][2]
trainingDFM[1]
?dictionary
gram4Freq <- docfreq(trainingDFM)
head(gram4Freq)
length(gram4Freq)
sum(gram4Freq)
head(sort(gram4Freq))
?sort
head(sort(gram4Freq, decreasing = TRUE))
trainingDFM4 <- dfm(trainingData, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
trainingDFM4 <- dfm(trainingData[1:10000], toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
trainingDFM4 <- dfm(trainingData[1:100000], toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
gram4Freq <- docfreq(trainingDFM)
gram4Freq[1]
head(names(gram4Freq))
?paste0
substr("abcdefg", 1, 4)
nm <- names(gram4Freq)
head(nm)
(substr(nm, 1, 3) == "the")
nm[substr(nm, 1, 3) == "the"]
search = "one_of_the_"
nm <- nm[substr(nm, 1, length(search)) == search]
nm
nm <- names(gram4Freq)
head(nm)
search = "you_need_is"
nm <- nm[substr(nm, 1, length(search)) == search]
nm
nm <- names(gram4Freq)
head(nm)
nm[substr(nm, 1, length(search)) == search]
nm[substr(nm, 1, 6 == "second"]
nm[substr(nm, 1, 6 == "second")]
nm[substr(nm, 1, 6) == "second"]
search
nm[substr(nm, 1, length(search)) == search]
length(search)
nm[substr(nm, 1, nchar(search)) == search]
which(substr(nm, 1, nchar(search)) == search)
predict3 <- function (docFreq, word1, word2, word3) {
name4 <- names(docFreq)
search <- paste0(word1, "_", word2, "_", word3, "_")
indices <- which(substr(name4, 1, nchar(search)) == search)
result <- sort(docFreq[indices], decreasing = TRUE)
result <- result / sum(result)
result
}
predict3(gram4Freq, "you", "need", "is")
predict3(gram4Freq, "i", "love", "you")
predict3(gram4Freq, "a", "case", "of")
predict3(gram4Freq, "would", "mean", "the")
predict3(gram4Freq, "make", "me", "the")
predict3(gram4Freq, "date", "at", "the")
tail(gram4Freq)
trainingDFM4 <- dfm(trainingData[1:500000], toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
gram4Freq <- docfreq(trainingDFM4)
predict3(gram4Freq, "a", "case", "of")
predict3 <- function (docFreq, word1, word2, word3) {
name4 <- names(docFreq)
search <- paste0(word1, "_", word2, "_", word3, "_")
indices <- which(substr(name4, 1, nchar(search)) == search)
result <- sort(docFreq[indices], decreasing = TRUE)
result <- result / sum(result)
result[1:5]
}
predict3(gram4Freq, "a", "case", "of")
predict3(gram4Freq, "would", "mean", "the")
predict3(gram4Freq, "make", "me", "the")
predict3(gram4Freq, "struggling", "but", "the")
predict3(gram4Freq, "date", "at", "the")
predict3 <- function (docFreq, word1, word2, word3) {
name4 <- names(docFreq)
search <- paste0(word1, "_", word2, "_", word3, "_")
indices <- which(substr(name4, 1, nchar(search)) == search)
result <- sort(docFreq[indices], decreasing = TRUE)
result <- result / sum(result)
result[1:min(5, length(result)]
}
predict3 <- function (docFreq, word1, word2, word3) {
name4 <- names(docFreq)
search <- paste0(word1, "_", word2, "_", word3, "_")
indices <- which(substr(name4, 1, nchar(search)) == search)
result <- sort(docFreq[indices], decreasing = TRUE)
result <- result / sum(result)
result[1:min(5, length(result))]
}
predict3(gram4Freq, "date", "at", "the")
predict3(gram4Freq, "be", "on", "my")
predict3(gram4Freq, "in", "quite", "some")
predict3(gram4Freq, "with", "his", "little")
predict3(gram4Freq, "faith", "during", "the")
predict3(gram4Freq, "you", "must", "be")
trainingDFM4 <- dfm(trainingData, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=4)
gram4Freq <- docfreq(trainingDFM4)
saveRDS(gram4Freq, "gram4Freq.rds")
saveRDS(trainingDFM4, "trainingDFM4.rds")
rm(trainingDFM)
rm(trainingDFM4)
gc()
head(gram4Freq)
length(gram4Freq)
length(gram4Freq[gram4Freq != 1])
predict3(gram4Freq, "a", "case", "of")
predict3(gram4Freq, "would", "mean", "the")
predict3(gram4Freq, "struggling", "but", "the")
predict3(gram4Freq, "date", "at", "the")
predict3(gram4Freq, "be", "on", "my")
substr("abc", 1)
predict3 <- function (docFreq, word1, word2, word3) {
name4 <- names(docFreq)
search <- paste0(word1, "_", word2, "_", word3, "_")
indices <- which(substr(name4, 1, nchar(search)) == search)
result <- sort(docFreq[indices], decreasing = TRUE)
if (length(result) > 0) {
result <- result / sum(result)
result <- result[1:min(5, length(result))]
nm <- names(result)
names(result) <- substr(nm, nchar(search) + 1, nchar(nm))
}
result
}
predict3(gram4Freq, "be", "on", "my")
dropSmallNumbers <- function (docFreq, cutoff) {
docFreq[docFreq > cutoff]
}
gram4FreqDrop1 <- dropSmallNumbers(gram4Freq, 1)
rm(gram4Freq)
gc()
rm(twitter)
rm(blog)
rm(news)
rm(trainTwitter)
rm(trainNews)
rm(trainBlog)
rm(testBlog)
rm(testNews)
rm(testTwitter)
rm(search)
rm (nm)
gc()
predict3(gram4FreqDrop1, "in", "quite", "some")
predict3(gram4FreqDrop1, "with", "his", "little")
predict3(gram4FreqDrop1, "faith", "during", "the")
predict3(gram4FreqDrop1, "you", "must", "be")
saveRDS(gram4FreqDrop1, "gram4FreqDrop1.rds")
stopwords(kind = "english", verbose = FALSE)
predict3(gram4FreqDrop1, "i'd", "like", "to")
predict3(gram4FreqDrop1, "to", "the", "grocery")
predict3(gram4FreqDrop1, "killed", "by", "an")
trainingDFM3 <- dfm(trainingData, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=3)
source("newAnalysis.R")
gram3Freq <- docfreq(trainingDFM3)
saveRDS(gram3Freq, "gram3Freq.rds")
rm(trainingDFM3)
gc()
gram3FreqDrop1 <- dropSmallNumbers(gram3Freq, 1)
saveRDS(gram3FreqDrop1, "gram3FreqDrop1.rds")
rm(gram3Freq)
gc()
predict2(gram3FreqDrop1, "case", "of")
predict2 <- function (docFreq3, word1, word2) {
name3 <- names(docFreq3)
search <- paste0(word1, "_", word2, "_")
indices <- which(substr(name3, 1, nchar(search)) == search)
result <- sort(docFreq3[indices], decreasing = TRUE)
if (length(result) > 0) {
result <- result / sum(result)
result <- result[1:min(3, length(result))]
nm <- names(result)
names(result) <- substr(nm, nchar(search) + 1, nchar(nm))
}
result
}
predict1 <- function (docFreq2, word1) {
name2 <- names(docFreq2)
search <- paste0(word1, "_", word2, "_")
indices <- which(substr(name2, 1, nchar(search)) == search)
result <- sort(docFreq2[indices], decreasing = TRUE)
if (length(result) > 0) {
result <- result / sum(result)
result <- result[1:min(3, length(result))]
nm <- names(result)
names(result) <- substr(nm, nchar(search) + 1, nchar(nm))
}
result
}
predict2(gram3FreqDrop1, "case", "of")
predict2(gram3FreqDrop1, "mean", "the")
predict2(gram3FreqDrop1, "me", "the")
predict2(gram3FreqDrop1, "but", "the")
predict2(gram3FreqDrop1, "at", "the")
predict2(gram3FreqDrop1, "on", "my")
predict2(gram3FreqDrop1, "quite", "some")
predict2(gram3FreqDrop1, "his", "little")
predict2(gram3FreqDrop1, "during", "the")
predict2(gram3FreqDrop1, "must", "be")
rm(testData)
rm(trainingData)
trainingDFM2 <- dfm(trainingData, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE, language="english", ngrams=2)
saveRDS(trainingDFM2, "trainingDFM2.rds")
trainingData <- readRDS("trainingData.rds")
source("newAnalysis.R")
gram2Freq <- docfreq(trainingDFM2)
saveRDS(gram2Freq, "gram2Freq.rds")
gram2FreqDrop1 <- dropSmallNumbers(gram2Freq, 1)
saveRDS(gram2FreqDrop1, "gram2FreqDrop1.rds")
rm(trainingDFM2)
rm(gram2Freq)
gc()
rm(trainingData)
gc()
rm(gram2FreqDrop1)
rm(gram3FreqDrop1)
rm(gram4FreqDrop1)
gc()
library(quanteda)
?stopwords
stopwords("english")
source("newAnalysis.R")
gc()
gramClean3Freq <- docfreq(trainingCleanDFM3)
saveRDS(gramClean3Freq, "gramClean3Freq.rds")
gramClean3FreqDrop1 <- dropSmallNumbers(gramClean3Freq, 1)
saveRDS(gramClean3FreqDrop1, "gramClean3FreqDrop1")
rm(trainingCleanDFM3)
predict("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", gramClean3FreqDrop1, 2)
source("prediction.R")
predict("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", gramClean3FreqDrop1, 2)
text <- "When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd"
tokens <- removeFeatures(tokenize(text, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE), stopwords("english"))
tokens
tokens <- tokens[(length(tokens) - ngram + 1):length(tokens)]
tokens <- tokens[(length(tokens) - 2 + 1):length(tokens)]
tokens
tokens <- removeFeatures(tokenize(text, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE), stopwords("english"))
tokens[[1]]
tokens <- removeFeatures(tokenize(text, removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE, removeTwitter = FALSE), stopwords("english"))[[1]]
tokens
tokens <- tokens[(length(tokens) - 2 + 1):length(tokens)]
tokens
predict("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", gramClean3FreqDrop1, 2)
source("prediction.R")
predict("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", gramClean3FreqDrop1, 2)
gramClean3FreqDrop1[1:5]
predict("yes prime and or minister", gramClean3FreqDrop1, 2)
predict("moment and right and and now", gramClean3FreqDrop1, 2)
predict("I moment and or right", gramClean3FreqDrop1, 2)
predict("maya and and never or I'd get", gramClean3FreqDrop1, 2)
predict("may and and I'd never", gramClean3FreqDrop1, 2)
rm(gramClean3Freq)
rm(gramClean3FreqDrop1)
rm(trainingData)
source("newAnalysis.R")
gc()
gram5Freq <- docfreq(trainingDFM5)
saveRDS(gram5Freq, "gram5Freq.rds")
rm(trainingData)
rm(trainingDFM5)
gc()
gram5FreqDrop1 <- dropSmallNumbers(gram5Freq, 1)
saveRDS(gram5FreqDrop1, "gram5FreqDrop1.rds")
rm(gram5Freq)
gc()
source("predict.R")
source("prediction.R")
predict("want to be the air for you. I'll be there for you, I'd live and I'd", gram5FreqDrop1, 4)
source("prediction.R")
predict("he started telling me about his", gram5FreqDrop1, 4)
rm(gram5FreqDrop1)
rm(text)
rm(tokens)
install.packages("quanteda", dependencies=TRUE)
install.packages("shiny", dependencies=TRUE)
?page
library(shiny)
?page
?shinyUI
runApp("prediction_app")
runApp("prediction_app")
runApp("prediction_app")
runApp("prediction_app")
runApp("prediction_app")
runApp("prediction_app")
runApp("prediction_app")
?isolate
runApp("prediction_app")
runApp("prediction_app")
?withProgress
runApp("prediction_app")
runApp("prediction_app")
